{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala implicits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicits in Scala are also pretty ubiquous. For instance, we can find them in the Scala standard library, and two popular frameworks: akka streams and Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![futureapi](../misc/futureapi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![futureapply](../misc/futureapply.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![futurefoldleft](../misc/futurefoldleft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![akka-flow](../misc/akka-api-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![akka-flow-runwith](../misc/akka-api-flow-runwith.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark-dataset](../misc/spark-api-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset-flatmap](../misc/spark-api-dataset-flatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark-groupbykey](../misc/spark-api-dataset-groupbykey.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicits as term inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw that the Scala compiler can infer type parameters in calls to generic functions. For instance, given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we may call our function passing explicitly the type parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we may omit the type parameter and let the Scala compiler to infer its proper value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can _values_ also be inferred by the Scala compiler? Yes, they can! For instance, in order to create an asynchronous computation using Scala `Future`s, we need to pass its `ExecutionContext`, i.e. the component that will be in charge of executing it. This is the signature of the factory method: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![futureapply](../misc/futureapply.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the execution context parameter is maked as `implicit` which means that the Scala compiler, in principle, can infer the value of that parameter for us. Of course, we can also pass the parameter explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.concurrent.{Future, ExecutionContext}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but the idiomatic way to write this invocation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooops. We forgot an important detail. In order for Scala to infer the type parameter we didn't have to do anything: Scala infers the type `Int` just from the type of the code to be executed. However, in order to infer the execution context, Scala needs some help in the form of `implicit` declarations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ec: ExecutionContext = ExecutionContext.global\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sum, if we want Scala to infer the value of some parameter we mark it as `implicit`. If we want some value to be used as the implicit parameter of a given type, we declare that value as `implicit`. But be careful not to declare more than one implicit value for a given type, at least at the same priority level. Otherwise, Scala complains of ambiguity. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ec2: ExecutionContext = ExecutionContext.global\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned in passing, Scala looks for implicits at severals levels of priority. You can find [here](https://docs.scala-lang.org/tutorials/FAQ/finding-implicits.html) a detailed explanation of these levels, which, besides the current scope, include companion objects of the type (and subtypes involved), ancestors of companion objects, etc. For instance, since implicits found in super-classes have less priority the following example compiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mScope3\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object Scope3{\n",
    "    import Scope1._\n",
    "    implicit val ec2: ExecutionContext = ExecutionContext.global\n",
    "\n",
    "    Future(1)        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicits for context parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example shows a very common use case of implicit parameters: to inject runtime dependencies, i.e. contextual parameters that are needed in order to execute our code. But why is this useful at all? Let's pretend that implicits are not available, and see what happens with the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(a: Int): Boolean = \n",
    "    bar(a).length > 0\n",
    "\n",
    "def bar(a: Int): String = \n",
    "    a.toString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major kinds of parameters in this dummy example: parameters `a` and `b`, and the execution context `ec`. These are very different in the sense that the former ones pertain to the business logic (so to speak) of the function, whereas the latter is concerned with a different aspect altogether (execution, in particular) and will be typically passed through unchanged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we add implicits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(a: Int)(ec: ExecutionContext): Future[Boolean] = \n",
    "    bar(a)(ec).map(_.length > 0)(ec)\n",
    "\n",
    "def bar(a: Int)(ec: ExecutionContext): Future[String] = \n",
    "    Future(a.toString)(ec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the programmer is no longer in charge of the repetitive task of passing the execution context: the compiler does that for her. This means more concise code and better separation of concerns. Rightly, the signatures still refer to the `ExecutionContext`, which means that this decoupling between logic and interpration is not fully completed. Fortunately, Scala 3 will feature *implicit function types*, which help significantly in this regard. Alternatively, we can also embrace the `Reader` monad ... but this is for other session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional implicit values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find a similar use implicits for injecting context paremeters in Akka stream. Let's analyse this example with some more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`com.typesafe.akka::akka-stream:2.6.4`\n",
    "import java.time._\n",
    "import scala.concurrent._, duration._\n",
    "import akka._\n",
    "import akka.actor._\n",
    "import akka.stream._\n",
    "import akka.stream.scaladsl._\n",
    "\n",
    "repl.pprinter() = repl.pprinter().copy(defaultHeight = 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we have this pipeline that simply generates a stream of integer values which are then printed in the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a declarative program. If we want our pipeline to be executed we need to `run` it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooops (again). In order to run a pipeline we need a `Materializer`: the component which will be in charge of executing the pipeline (it thus plays a similar role to the `ExecutionContext` of future-based computations). This is explicitly stated in the `run` signature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![run](../misc/runnablegraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![runnablegraphrun](../misc/runnablegraphrun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard `Materializer` that is provided by the akka stream library executes pipelines by means of an actor system. So, we first need to create an `ActorSystem`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can now run our pipeline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, wait, `run` needs an implicit value of the type `Materializer`, and we defined an implicit value of type `ActorSystem`. How did Scala infer an implicit value for the former type? The answer lies in the following declaration located in the companion object of the `Materializer` class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "\n",
    "object Materializer {\n",
    "\n",
    "  implicit def matFromSystem(\n",
    "      implicit provider: ClassicActorSystemProvider): Materializer =\n",
    "    ???\n",
    "\n",
    "}\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`matFromSystem` is called a _conditional implicit value_, meaning that Scala can construct an implicit value for the `Materializer` type, _provided that_ it can infer an implicit value for a `ClassicActorSystemProvider`. In our example, it certainly can, since we declared an implicit value of the type `ActorSystem` (which extends the trait `ClassicActorSystemProvider`). Ok, but how did the Scala compiler find that conditional implicit value?, because it was certainly not in scope. Right, but `matFromSystem` is defined in the _companion object_ of the `Materializer` type, and we already said Scala looks automatically for implicits there, among other places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Implicits for extension methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common use case for implicits is the implementation of extension methods: there is a class for which we have no control, or no interest in modifying its definition, and, yet, we would like to add some new methods. The Scala standard library of Scala has good examples of this pattern. For instance, as we already know, the `String` class comes actually from Java:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `java.lang.String` does not declares a `map` method. How then can we make this call?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer lies in the following declaration in the `Predef.scala` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "  implicit def augmentString(x: String): StringOps = \n",
    "      new StringOps(x)\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a conditional implicit value, because the argument `x` is not implicit. This is a so-called *implicit conversion* that allows the Scala compiler to convert a `String` into an object of type `StringOps`. This class is the one that actually provides the new method `map`. It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to extend the `String` class with this new method, we have to do something similar to what the Scala library does, i.e. provide an implicit conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern is so common that Scala 2.10 provided a special construct for it: *implicit classes* (you may also think of implicit classes in terms of the adapter OO pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStringOps(x: String){\n",
    "    def mymap(f: Char => Char): String = \n",
    "        \"dummy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we didn't need to write the implicit conversion function. Also, note that that we changed the name of the method. This is in order to avoid a compilation error due to multiple conversions available (a problem of _ambiguity_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicits for ad-hoc polymorphism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's come back to the use of implicits in the following signature of the Spark framework:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark-dataset](../misc/spark-api-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset-flatmap](../misc/datasetmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very briefly, objects of the class `Dataset` represent distributed dataset transformation programs. In this context, an `Encoder[T]` is used to convert JVM objects of type `T` from/to `Row`s, the dynamically typed internal representation of Spark. The definition of the `Encoder[T]` API does not mention, however, these conversion functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "trait Encoder[T] extends Serializable {\n",
    "\n",
    "  /** Returns the schema of encoding this type of object as a Row. */\n",
    "  def schema: StructType\n",
    "\n",
    "  /**\n",
    "   * A ClassTag that can be used to construct and Array to contain a collection of `T`.\n",
    "   */\n",
    "  def clsTag: ClassTag[T]\n",
    "}\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, the trait `Encoder[T]` defines a _type class_, namely the class of types that can be serialized/deserialized to/from Spark `Row`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first difference with the use case of context parameters (e.g. the `ExecutionContext` in futures) is that the implicit parameter `Encoder[U]` is related to the generic parameter `U`. What does this mean? Given that the signature is generic, we may intend to apply this method to any type `U`. However, this will only be possible if type `U` is internally supported by Spark, i.e. if there exists an implicit instance of the `Encoder` class for the type `U`. Therefore, this method is not really a case of parametric polymorphism, where there are no constraints over the type parameter. And it is not a case of subtype plymorphism either, since there are no subtype bounds on `U`. This type of polymorphism is named _ad-hoc polymorphism_ since the implementation depends on the particular, _ad-hoc_, implementation of the trait `Encoder` for the type `U`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scala offers some syntactic sugar to express this kind of polymorphism more succintly: so-called, _context bounds_. Using this feature, the `map` signature is written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "class Dataset[T]{\n",
    " def map[U : Encoder](func: T => U): Dataset[U] = \n",
    "   ...\n",
    "}\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's actually debatable if the `Encoder` example is a clear-cut case of ad-hoc polymorphism, since the encoder evidence does not affect the behaviour of the `map` method, but of the `Dataset` interpreters (actions, in particular). What we are actually doing is enforcing a constraint in the `Dataset` API, i.e. something nearer to the use case of implicits for checking type-level proofs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical use case for type classes is the following one: we want to aggregate elements of a list according to the specific, ad-hoc, aggregators of the different types. We may start from this signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but, clearly, we know nothing about `T`, in particular, about its aggregator. So, we extend the signature with the parameters needed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(elements: List[Int]): Int = \n",
    "    elements match {\n",
    "        case Nil => 0\n",
    "        case head :: tail => aggregate(tail) + head \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and, we can now, aggregate integers, strings, and whatever: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type classes come into play when we decide to pack the extra parameters in a special-purpose module, namely, `Aggregatable`, the class of types whose values can be aggregated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, now, we can write our signature as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate[T](elements: List[T])(zero: T, combine: (T, T) => T): T =\n",
    "    elements match {\n",
    "        case Nil => zero : T\n",
    "        case head :: tail => combine(head, aggregate(tail)(zero, combine))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, using context bounds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the signature is clearer now, but the body got somewhat polluted with `implicitly` calls. We can remove that noise with special-purpose syntax (using implicit classes). Last, the `Aggregatable` type class is, of course, the `Monoid` type class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we gain using type classes instead of plain function parameters? \n",
    "* More concise syntax at call sites\n",
    "* Improved reusability of instances\n",
    "* Improved testability (type classes laws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other implicits-related stuff we didn't have time to cover:\n",
    "* Type-level functions (doing type-level computing à la shapeless)\n",
    "* Standard type-level proofs (`<:<`, `=:=`, conversions, etc.)\n",
    "* Implicits in dotty (better separation of intent vs. mecanism, implicit function types,  special syntax, ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, think of your own use case. Don't use implicits without a clear-cut intent. \n",
    "* Exploit implicits for injecting run-time dependencies (i.e. context)\n",
    "* Exploit implicits whenever the adapter pattern comes to your mind \n",
    "* Exploit implicits for ad-hoc polymorphism (and avoid inheritance!)\n",
    "* Look forward to Scala 3.0 enhancements in regard to implicits\n",
    "* Be aware of cons in Scala 2.x (e.g. compilation times, misuse, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![downs](../misc/downtimes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "FILIP KŘIKAVA, HEATHER MILLER, JAN VITEK. [Scala Implicits Are Everywhere](https://arxiv.org/pdf/1908.07883.pdf)\n",
    "\n",
    "MARTIN ODERSKY. [Simplicitly: Foundations and Applications of Implicit Function Types](https://popl18.sigplan.org/details/POPL-2018-papers/85/Simplicitly-Foundations-and-Applications-of-Implicit-Function-Types)\n",
    "\n",
    "[Implicits in Dotty](https://dotty.epfl.ch/docs/reference/contextual/motivation.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
